{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import contextlib\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import polars as pl\n",
    "from pydoll.browser.chrome import Chrome\n",
    "from pydoll.browser.options import Options\n",
    "from pydoll.constants import By\n",
    "from pydoll.element import WebElement\n",
    "from tqdm import tqdm\n",
    "\n",
    "SCRAPE_CHUNK_PAGES = 60\n",
    "repo_path = r\"D:\\projets_python_ssd\\Sencrop\\perso\\MH_Wilds_tools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headless = False\n",
    "chrome_user_data = os.path.join(os.getcwd(), \"chrome_user_data\")\n",
    "\n",
    "options = Options()\n",
    "if headless:\n",
    "    options.add_argument(\"--headless\")\n",
    "# options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--mute-audio\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "# options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "options.add_argument(f\"--user-data-dir={chrome_user_data}\")\n",
    "options.add_argument(\"--profile-directory=Default\")\n",
    "# options.add_argument(\"--remote-debugging-port=9223\")\n",
    "options.add_argument(\"--referer=https://patreon.com\")\n",
    "# options.set_capability(\"goog:loggingPrefs\", {\"performance\": \"ALL\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_quest_rewards(\n",
    "    quest: dict[str, str], browser, root_url: str\n",
    ") -> list[dict[str, str]]:\n",
    "    href = quest[\"href\"]\n",
    "    page = await browser.get_page()\n",
    "    await page.go_to(url=f\"{root_url}{href}\")\n",
    "\n",
    "    recompenses_element = await page.find_element(\n",
    "        By.XPATH,\n",
    "        \"//h3[contains(text(), 'Récompenses')]\",\n",
    "    )\n",
    "    parent_element = await recompenses_element.find_element(By.XPATH, \"..\")\n",
    "\n",
    "    recompenses_list = []\n",
    "\n",
    "    recompenses = await parent_element.find_elements(By.TAG_NAME, \"tr\")\n",
    "    for recompense in recompenses:\n",
    "        value = await recompense.find_elements(By.TAG_NAME, \"td\")\n",
    "        value = value[0]\n",
    "        item = await value.find_element(By.TAG_NAME, \"a\")\n",
    "        item = await item.get_element_text()\n",
    "        quantity = await value.get_element_text()\n",
    "        quantity = quantity.replace(item, \"\").strip().replace(\"x\", \"\")\n",
    "        match quantity:\n",
    "            case \"\":\n",
    "                quantity = 1\n",
    "            case _:\n",
    "                quantity = int(quantity)\n",
    "        recompenses_list.append({\"item\": item, \"quantity\": quantity})\n",
    "    quest[\"rewards\"] = recompenses_list\n",
    "    await page.close()\n",
    "    return quest\n",
    "\n",
    "\n",
    "async def get_quest_details(quest: WebElement) -> dict[str, str]:\n",
    "    quest_name_element = await quest.find_elements(By.TAG_NAME, \"td\")\n",
    "    quest_name_element = quest_name_element[0]\n",
    "    quest_name = await quest_name_element.find_element(By.TAG_NAME, \"a\")\n",
    "    return {\n",
    "        \"name\": await quest_name.get_element_text(),\n",
    "        \"href\": quest_name.get_attribute(\"href\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url = \"https://mhwilds.kiranico.com\"\n",
    "quest_page = f\"{root_url}/fr/data/missions\"\n",
    "\n",
    "async with Chrome(options=options) as browser:\n",
    "    await browser.start()\n",
    "    page = await browser.get_page()\n",
    "    await page.go_to(url=quest_page, timeout=10)\n",
    "\n",
    "    quest_scroll_element = await page.find_element(\n",
    "        By.XPATH, \"/html/body/div[1]/div/div/div[2]/div/div[2]\"\n",
    "    )\n",
    "    quests = await quest_scroll_element.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "    all_quests = await asyncio.gather(*[get_quest_details(quest) for quest in quests])\n",
    "\n",
    "    all_quests_rewards = []\n",
    "    for i in tqdm(range(0, len(all_quests) + 1, SCRAPE_CHUNK_PAGES)):\n",
    "        chunk = all_quests[i : i + SCRAPE_CHUNK_PAGES]\n",
    "        quest_rewards_chunk = await asyncio.gather(\n",
    "            *(\n",
    "                get_quest_rewards(\n",
    "                    quest=quest,\n",
    "                    browser=browser,\n",
    "                    root_url=root_url,\n",
    "                )\n",
    "                for quest in chunk\n",
    "            )\n",
    "        )\n",
    "        all_quests_rewards.extend(quest_rewards_chunk)\n",
    "    quest_df = (\n",
    "        pl.DataFrame(all_quests_rewards)\n",
    "        #\n",
    "        .explode(\"rewards\")\n",
    "        .with_columns(\n",
    "            pl.col(\"rewards\").struct.field(\"item\").alias(\"item\"),\n",
    "            pl.col(\"rewards\").struct.field(\"quantity\").alias(\"quantity\"),\n",
    "        )\n",
    "        .drop(\"rewards\")\n",
    "        .filter(pl.col(\"item\").is_not_null())\n",
    "        .sort(\"item\", \"quantity\", descending=[False, True])\n",
    "        .to_pandas()\n",
    "        .get([\"name\", \"item\", \"quantity\"])\n",
    "    )\n",
    "    quest_df.to_parquet(os.path.join(repo_path, \"data\", \"quests.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape armors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_armor_data(\n",
    "    browser,\n",
    "    root_url: str,\n",
    "    href: str,\n",
    ") -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract armor data from a web page, parsing talent table rows to collect piece details.\n",
    "\n",
    "    Navigates to a specific URL, finds the talent table, and extracts information about\n",
    "    each armor piece including its name, jewel levels, and associated talents.\n",
    "\n",
    "    Returns:\n",
    "        list[dict[str, str]]: A list of dictionaries containing armor piece details with\n",
    "        keys 'piece', 'name', 'jewels', and 'talents'.\n",
    "    \"\"\"\n",
    "    page = await browser.get_page()\n",
    "    await page.go_to(url=f\"{root_url}{href}\")\n",
    "\n",
    "    # Get skill table\n",
    "    talent_table = await page.find_element(\n",
    "        By.XPATH,\n",
    "        '//th[contains(text(), \"Talents de l\\'équipement\")]',\n",
    "    )\n",
    "    talent_table = await talent_table.find_element(By.XPATH, \"..\")\n",
    "    talent_table = await talent_table.find_element(By.XPATH, \"..\")\n",
    "\n",
    "    all_pieces = []\n",
    "    talent_table_rows = (await talent_table.find_elements(By.TAG_NAME, \"tr\"))[1:]\n",
    "    for talent_table_row in talent_table_rows:\n",
    "        piece, name, jewels, talent = await talent_table_row.find_elements(\n",
    "            By.TAG_NAME, \"td\"\n",
    "        )\n",
    "        piece, name, jewels = await asyncio.gather(\n",
    "            *[x.get_element_text() for x in (piece, name, jewels)]\n",
    "        )\n",
    "\n",
    "        # Extract jewel levels\n",
    "        all_jewels = {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0}\n",
    "        for char in jewels.replace(\"[\", \"\").replace(\"]\", \"\"):\n",
    "            all_jewels[char] += 1\n",
    "\n",
    "        # Extract talent information\n",
    "        all_talents = []\n",
    "        with contextlib.suppress(Exception):\n",
    "            talents = await talent.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "        for _talent in talents:\n",
    "            talent_name = await _talent.get_element_text()\n",
    "            talent_level = int(talent_name.split(\"+\")[-1])\n",
    "            talent_name = talent_name.split(\"+\")[:-1]\n",
    "            talent_name = \"+\".join(talent_name).strip()\n",
    "            all_talents.append(\n",
    "                {\n",
    "                    \"talent_name\": talent_name,\n",
    "                    \"talent_level\": talent_level,\n",
    "                }\n",
    "            )\n",
    "            piece_dict = {\n",
    "                \"piece\": piece,\n",
    "                \"name\": name,\n",
    "                \"jewels\": all_jewels,\n",
    "                \"talents\": all_talents,\n",
    "            }\n",
    "            all_pieces.append(piece_dict)\n",
    "    await page.close()\n",
    "    return all_pieces\n",
    "\n",
    "\n",
    "root_url = \"https://mhwilds.kiranico.com\"\n",
    "armor_page = f\"{root_url}/fr/data/armor-series\"\n",
    "\n",
    "async with Chrome(options=options) as browser:\n",
    "    await browser.start()\n",
    "    page = await browser.get_page()\n",
    "    await page.go_to(armor_page)\n",
    "\n",
    "    scroll_element = await page.find_element(By.TAG_NAME, \"table\")\n",
    "    hrefs = await scroll_element.find_elements(By.TAG_NAME, \"a\")\n",
    "    hrefs = [element.get_attribute(\"href\") for element in hrefs]\n",
    "\n",
    "    all_armor_data = []\n",
    "    for i in tqdm(range(0, len(hrefs) + 1, SCRAPE_CHUNK_PAGES)):\n",
    "        chunk = hrefs[i : i + SCRAPE_CHUNK_PAGES]\n",
    "        armor_data_chunk = await asyncio.gather(\n",
    "            *(\n",
    "                extract_armor_data(\n",
    "                    browser=browser,\n",
    "                    root_url=root_url,\n",
    "                    href=href,\n",
    "                )\n",
    "                for href in chunk\n",
    "            )\n",
    "        )\n",
    "        all_armor_data.extend(armor_data_chunk)\n",
    "\n",
    "    all_armor_data = list(itertools.chain.from_iterable(all_armor_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "armor_pieces = (\n",
    "    pl.DataFrame(all_armor_data)\n",
    "    #\n",
    "    .explode(\"talents\")\n",
    "    .with_columns(\n",
    "        pl.col(\"talents\").struct.field(\"talent_name\").alias(\"talent_name\"),\n",
    "        pl.col(\"talents\").struct.field(\"talent_level\").alias(\"talent_level\"),\n",
    "    )\n",
    "    .drop(\"talents\")\n",
    "    .with_columns(\n",
    "        *[\n",
    "            pl.col(\"jewels\").struct.field(jwl_lvl).alias(f\"jewel_{jwl_lvl}\")\n",
    "            for jwl_lvl in [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "        ]\n",
    "    )\n",
    "    .drop(\"jewels\")\n",
    ")\n",
    "armor_pieces.write_parquet(os.path.join(repo_path, \"data\", \"armor_pieces.parquet\"))\n",
    "armor_pieces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url = \"https://mhwilds.kiranico.com\"\n",
    "charms_url = f\"{root_url}/fr/data/charms\"\n",
    "\n",
    "\n",
    "async def extract_charm_row_data(charm_element: WebElement) -> dict[str, str]:\n",
    "    charm_name = await charm_element.find_element(By.TAG_NAME, \"a\")\n",
    "    charm_name = await charm_name.get_element_text()\n",
    "    charm_href = (await charm_element.find_element(By.TAG_NAME, \"a\")).get_attribute(\n",
    "        \"href\"\n",
    "    )\n",
    "    return {\"name\": charm_name, \"href\": charm_href}\n",
    "\n",
    "\n",
    "async def extract_charm_data(\n",
    "    browser,\n",
    "    root_url: str,\n",
    "    charm_element: dict[str, str],\n",
    ") -> dict[str, str]:\n",
    "    page = await browser.get_page()\n",
    "    await page.go_to(f\"{root_url}{charm_element['href']}\")\n",
    "\n",
    "    talent_table = await page.find_element(By.TAG_NAME, \"tbody\")\n",
    "    talents = await talent_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "    charm_talents = []\n",
    "    for talent in talents:\n",
    "        name, lvl, desc = await talent.find_elements(By.TAG_NAME, \"td\")\n",
    "        name = await name.get_element_text()\n",
    "        lvl = await lvl.get_element_text()\n",
    "        desc = await desc.get_element_text()\n",
    "\n",
    "        lvl = int(lvl.replace(\"Lv\", \"\").strip())\n",
    "        charm_talents.append({\"name\": name, \"lvl\": lvl})\n",
    "    charm_element[\"talents\"] = charm_talents\n",
    "    await page.close()\n",
    "    return charm_element\n",
    "\n",
    "\n",
    "async with Chrome(options=options) as browser:\n",
    "    await browser.start()\n",
    "    page = await browser.get_page()\n",
    "    await page.go_to(url=charms_url)\n",
    "\n",
    "    scroll_element = await page.find_element(By.TAG_NAME, \"table\")\n",
    "    charm_elements = await page.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "    all_charms = await asyncio.gather(\n",
    "        *[extract_charm_row_data(charm_element) for charm_element in charm_elements]\n",
    "    )\n",
    "\n",
    "    all_charm_data = []\n",
    "    for i in tqdm(range(0, len(all_charms) + 1, SCRAPE_CHUNK_PAGES)):\n",
    "        chunk = all_charms[i : i + SCRAPE_CHUNK_PAGES]\n",
    "        charm_data_chunk = await asyncio.gather(\n",
    "            *[\n",
    "                extract_charm_data(\n",
    "                    browser=browser,\n",
    "                    root_url=root_url,\n",
    "                    charm_element=charm_element,\n",
    "                )\n",
    "                for charm_element in chunk\n",
    "            ]\n",
    "        )\n",
    "        all_charm_data.extend(charm_data_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charms_data = (\n",
    "    pl.DataFrame(all_charms)\n",
    "    #\n",
    "    .explode(\"talents\")\n",
    "    .with_columns(\n",
    "        pl.col(\"talents\").struct.field(\"name\").alias(\"talent_name\"),\n",
    "        pl.col(\"talents\").struct.field(\"lvl\").alias(\"talent_lvl\"),\n",
    "    )\n",
    "    .drop(\"talents\")\n",
    "    .sort(\"talent_name\", \"talent_lvl\", descending=[False, True])\n",
    ")\n",
    "charms_data.write_parquet(os.path.join(repo_path, \"data\", \"charms.parquet\"))\n",
    "charms_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
